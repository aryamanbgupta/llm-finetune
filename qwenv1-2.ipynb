{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements-lock.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import argparse, json, random, os, warnings, math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Model and Tokenizer Setup\n",
    "model_name  = \"Qwen/Qwen-1_8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = '<|endoftext|>'\n",
    "\n",
    "# Simplified CUDA-focused device handling\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "device_map = \"auto\" if torch.cuda.is_available() else None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype = dtype,\n",
    "    device_map = device_map,\n",
    "    trust_remote_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Token Definitions\n",
    "'''\n",
    "new_tokens = [\"<O_0>\", \"<O_1>\", \"<O_2>\", \"<O_3>\", \"<O_4>\",  \"<O_6>\",  \"<O_W>\"]\n",
    "num_added = tokenizer._tokenize.add_tokens(new_tokens)\n",
    "print(num_added)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": new_tokens})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(\"Added tokens:\", new_tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "if all(i != tokenizer.unk_token_id for i in ids):\n",
    "    print(\"added successfully\")\n",
    "else:\n",
    "    print(\"failed\")\n",
    "\n",
    "for token in new_tokens:\n",
    "    if token not in tokenizer.get_vocab():\n",
    "        print(f\"Warning: Token {token} may not have been added properly\")\n",
    "'''\n",
    "#use placeholder tokens for the outcomes\n",
    "EXTRA_BASE = 40\n",
    "OUTCOMES = [\"0\", \"1\", \"2\", \"3\", \"4\", \"6\", \"WICKET\"]\n",
    "\n",
    "OUTCOME2TOK = {o:f\"<|extra_{EXTRA_BASE+i}|>\" for i,o in enumerate(OUTCOMES)}\n",
    "TOK2OUTCOME = {v:k for k,v in OUTCOME2TOK.items()}\n",
    "\n",
    "for k,v in OUTCOME2TOK.items():\n",
    "    print(f\" {k:7s} -> {v}\")\n",
    "\n",
    "#sanity check for tokenizer working\n",
    "for tok in  OUTCOME2TOK.values():\n",
    "    ids = tokenizer(tok)[\"input_ids\"]\n",
    "    #print(tok,ids)\n",
    "    assert len(ids) == 1, f\"{tok} splits into multiple tokens\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Dataset Building Function\n",
    "def build_dataset(jsonl_path: str, tokenizer, max_len: int =64):\n",
    "\n",
    "    raw_ds = load_dataset(\"json\", data_files=jsonl_path, split = \"train\")\n",
    "    def _to_features(example):\n",
    "        tgt_tok = OUTCOME2TOK[example[\"target\"]]\n",
    "        text = example[\"prompt\"].rstrip() + \" \" + tgt_tok\n",
    "        enc = tokenizer(text, truncation= True, padding = \"max_length\", max_length = max_len)\n",
    "        # Add labels for training (same as input_ids, but mask padding tokens)\n",
    "        enc[\"labels\"] = [\n",
    "            token_id if token_id != tokenizer.pad_token_id else -100 \n",
    "            for token_id in enc[\"input_ids\"]\n",
    "        ]\n",
    "        return enc\n",
    "    return raw_ds.map(_to_features, remove_columns= raw_ds.column_names)\n",
    "\n",
    "# ds = build_dataset('/Users/aryamangupta/CricML/llm-finetune/cricket_prompts.jsonl', tokenizer, max_len=128)\n",
    "# example = ds[0]\n",
    "# print(example[\"input_ids\"])\n",
    "# print(\"attention mask:\", example[\"attention_mask\"])\n",
    "# print (\"labels\", example[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: LoRA Configuration\n",
    "#setup LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules= \"all-linear\", #[\"c_attn\", \"c_proj\", \"w1\", \"w2\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Training Function\n",
    "#training loop\n",
    "def train_qwen (model, dataset, out_dir, epochs:int, batch: int):\n",
    "    args = TrainingArguments(\n",
    "        output_dir= out_dir,\n",
    "        per_device_train_batch_size= batch,\n",
    "        num_train_epochs= epochs,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        learning_rate= 3e-4,\n",
    "        fp16 = torch.cuda.is_available(),\n",
    "        logging_steps= 25,\n",
    "        save_strategy= \"epoch\",\n",
    "        report_to= \"none\",\n",
    "        dataloader_num_workers=2\n",
    "    )\n",
    "    trainer = Trainer(model=model, args=args, train_dataset= dataset)\n",
    "    trainer.train() \n",
    "    trainer.save_model(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Prediction Function\n",
    "def predict_dist(prompt:str, model, tokenizer):\n",
    "    model.eval()\n",
    "    ids = tokenizer(prompt, return_tensors = \"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**ids).logits[0, -1, :]\n",
    "    sel = tokenizer.convert_tokens_to_ids(list(OUTCOME2TOK.values()))\n",
    "    probs = torch.softmax(logits[sel], dim =-1).cpu().tolist()\n",
    "    return {o: p for o,p in zip(OUTCOMES, probs)}\n",
    "\n",
    "#test\n",
    "# input_ids = tokenizer(\"Vaibhav Arora bowling to Tristan Stubbs\", return_tensors = \"pt\").input_ids.to(model.device)\n",
    "# output = model.generate(input_ids)\n",
    "# print (tokenizer.decode(output[0], skip_special_tokens = True))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Training Execution\n",
    "# Set your parameters here\n",
    "data_path = \"cricket_prompts.jsonl\"  # Set to your JSONL file path\n",
    "epochs = 3\n",
    "batch_size = 2\n",
    "output_dir = \"qwen-cricket-peft\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    ds = build_dataset(data_path, tokenizer)\n",
    "    qwen_trainer = train_qwen(model, ds, output_dir, epochs, batch_size)\n",
    "    \n",
    "    #demo\n",
    "    demo = \"M Theekshana bowling to YBK Jaiswal, 7/0 after 1.4 overs, 1st innings, Pallekele International Cricket Stadium, 2024-07-30\"\n",
    "    dist = predict_dist (demo,model, tokenizer)\n",
    "    for k,v in dist.items():\n",
    "        print(f\" {k:7s} : {v:.3f}\")\n",
    "else:\n",
    "    print(\"Set data_path to your JSONL file to start training\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
